{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'data/train.csv'\n",
    "\n",
    "with open(FILENAME) as file:\n",
    "  lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "  df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "  df = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "  0: 1,\n",
    "  1: 2,\n",
    "  2: 3,\n",
    "  3: 4,\n",
    "  4: 5,\n",
    "  5: 6,\n",
    "  6: 7,\n",
    "  7: 8,\n",
    "  8: 9,\n",
    "  9: 10,\n",
    "  10: 11,\n",
    "  11: 12,\n",
    "  12: 13,\n",
    "  13: 14,\n",
    "  14: 15,\n",
    "  15: 16,\n",
    "  16: 17,\n",
    "  17: 18,\n",
    "  18: 19\n",
    "}\n",
    "label2id = {\n",
    "  1: 0,\n",
    "  2: 1,\n",
    "  3: 2,\n",
    "  4: 3,\n",
    "  5: 4,\n",
    "  6: 5,\n",
    "  7: 6,\n",
    "  8: 7,\n",
    "  9: 8,\n",
    "  10: 9,\n",
    "  11: 10,\n",
    "  12: 11,\n",
    "  13: 12,\n",
    "  14: 13,\n",
    "  15: 14,\n",
    "  16: 15,\n",
    "  17: 16,\n",
    "  18: 17,\n",
    "  19: 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodeLabel(labels):\n",
    "  classes = [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "  intLabels = [int(label) for label in labels.split(\" \")]\n",
    "  for label in intLabels:\n",
    "     classes[label - 1] = 1.\n",
    "  return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "token_type_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for idx, row in enumerate(df.itertuples()):\n",
    "    # df.at[row.Index, 'ImageID'] = normaliseImage(row.ImageID)\n",
    "    df.at[row.Index, 'Labels'] = oneHotEncodeLabel(row.Labels)\n",
    "    # df.at[row.Index, 'Caption'] = tokenise(row.Caption)\n",
    "    encodings = tokenizer(row.Caption, padding=\"max_length\", truncation=True, max_length=20)\n",
    "    input_ids.append(encodings['input_ids'])\n",
    "    token_type_ids.append(encodings['token_type_ids'])\n",
    "    attention_mask.append(encodings['attention_mask'])\n",
    "    # df['input_ids'].loc[0] = encoding['input_ids']\n",
    "    # print(df)\n",
    "    # print(idx)\n",
    "    # print(tokenizer.decode(encoding['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(3, 'input_ids', input_ids)\n",
    "df.insert(4, 'token_type_ids', token_type_ids)\n",
    "df.insert(5, 'attention_mask', attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ImageID                                             Labels  \\\n",
      "0   0.jpg  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1   1.jpg  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2   2.jpg  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3   3.jpg  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n",
      "4   4.jpg  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...   \n",
      "\n",
      "                                             Caption  \\\n",
      "0   Woman in swim suit holding parasol on sunny day.   \n",
      "1  A couple of men riding horses on top of a gree...   \n",
      "2  They are brave for riding in the jungle on tho...   \n",
      "3  a black and silver clock tower at an intersect...   \n",
      "4   A train coming to a stop on the tracks out side.   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 2450, 1999, 9880, 4848, 3173, 11498, 194...   \n",
      "1  [101, 1037, 3232, 1997, 2273, 5559, 5194, 2006...   \n",
      "2  [101, 2027, 2024, 9191, 2005, 5559, 1999, 1996...   \n",
      "3  [101, 1037, 2304, 1998, 3165, 5119, 3578, 2012...   \n",
      "4  [101, 1037, 3345, 2746, 2000, 1037, 2644, 2006...   \n",
      "\n",
      "                                      token_type_ids  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                      attention_mask  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  \"bert-base-uncased\",\n",
    "  problem_type=\"multi_label_classification\",\n",
    "  num_labels=19,\n",
    "  id2label=id2label,\n",
    "  label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2450,  1999,  9880,  4848,  3173, 11498, 19454,  2006, 11559,\n",
       "          2154,  1012,   102,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(df['input_ids'][0]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7139, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.2458,  0.0584, -0.8073,  0.0442, -0.5776,  0.6092, -0.4927,  0.3446,\n",
       "          0.2448, -0.5293, -0.5679, -0.4635, -0.3004,  0.1149,  0.4977,  0.7999,\n",
       "         -0.1824,  0.2172,  0.5422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=torch.tensor(df['input_ids'][0]).unsqueeze(0), labels=torch.tensor(df['Labels'][0]).unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 417.705MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
